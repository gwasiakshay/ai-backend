AI Backend â€“ Retrieval-Augmented Generation (RAG)

A production-style AI backend implementing a complete Retrieval-Augmented Generation (RAG) pipeline with clean architecture, async processing, and persistent memory.

Status: Step 4 (RAG) complete and intentionally frozen.

ğŸš€ What This Demonstrates

This project proves the ability to:

Design end-to-end AI systems, not just call LLM APIs

Build production-grade backends using FastAPI

Implement correct RAG (retrieval â†’ grounding â†’ generation)

Separate semantic memory from system state

Integrate LLM providers safely and debuggably

Operate async pipelines with background workers

ğŸ§  System Overview

Flow

Document â†’ Chunking â†’ Embeddings â†’ Vector DB â†’ Retrieval â†’ Context â†’ LLM Answer


Key Design Choices

Thin API routes, logic in services

Async ingestion via background worker

Vector DB for semantic search

Relational DB for metadata & job state

Provider-agnostic LLM abstraction

ğŸ—ï¸ Tech Stack

FastAPI â€“ API layer

Background Worker â€“ async ingestion

Chroma â€“ vector database

PostgreSQL â€“ metadata & job tracking

OpenRouter â€“ LLM + embeddings

Docker Compose â€“ local orchestration

ğŸ”Œ Core API Endpoints
Ingest
POST /ingest


Triggers async document ingestion, chunking, embedding, and storage.

Retrieve & Generate
POST /retrieve


Performs semantic retrieval and returns a context-grounded LLM response.

Health
GET /health


Swagger UI available at:

/docs

ğŸ—„ï¸ Persistence Model

Chroma (Vector DB)
Stores embeddings for semantic similarity search

PostgreSQL (Relational DB)
Stores ingestion jobs, processing state, and metadata

This avoids the common anti-pattern of storing system state in vector databases.

âœ… RAG Capability Coverage

This repository fully implements:

Document ingestion

Text chunking

Embedding generation

Vector storage (Chroma)

Metadata layer (Postgres)

Semantic retrieval

Context-grounded generation

ğŸ‘‰ Complete RAG loop implemented and verified

ğŸ“Œ Project Status

ğŸ”’ Frozen at Step 4 (RAG)

Evaluation, observability, and monitoring (Step 5) are intentionally excluded to keep this repository a clean RAG baseline.

âš™ï¸ Run Locally
docker compose up --build


Environment variable required:

OPENAI_API_KEY=sk-or-xxxxxxxxxxxxxxxx


(OpenRouter key)

ğŸ¯ Why This Project Exists

This is a portfolio and learning milestone project built to demonstrate applied AI engineering skills, system design clarity, and real-world backend patterns â€” not a toy demo or UI app.
