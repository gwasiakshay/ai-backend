AI Backend â€“ Retrieval-Augmented Generation (RAG)

This repository contains a production-style AI backend that implements a complete Retrieval-Augmented Generation (RAG) pipeline.

The project is built to demonstrate applied AI engineering principles, focusing on clean architecture, separation of concerns, and real-world backend patterns.

ğŸ“Œ Project Status

âœ… Step 4 (RAG) â€“ Complete and Frozen

This repository is intentionally frozen at Step 4 of the AI Engineer roadmap.
Evaluation, observability, and monitoring (Step 5) are not included by design.

ğŸ§  What This Project Does

At a high level, this backend:

Ingests raw text documents

Splits them into chunks

Generates embeddings

Stores embeddings in a vector database

Stores metadata and job state in PostgreSQL

Retrieves relevant context using semantic search

Generates grounded answers using an LLM

In short:

Documents â†’ Embeddings â†’ Vector Search â†’ Context â†’ LLM Answer

ğŸ—ï¸ Architecture Overview
Core Components

FastAPI â€“ REST API layer

Background Worker â€“ Asynchronous ingestion and processing

Chroma â€“ Vector database for semantic search

PostgreSQL â€“ Metadata and job state storage

OpenRouter â€“ LLM + embeddings provider

Docker Compose â€“ Local orchestration

Design Principles

Thin API routes

Business logic isolated in services

Clear separation between:

semantic memory (vectors)

system state (relational DB)

Async-safe ingestion pipeline

Provider-agnostic LLM abstraction

ğŸ“‚ Project Structure (Simplified)
ai-backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ routes/        # FastAPI endpoints
â”‚   â”œâ”€â”€ rag/           # Retrieval + generation logic
â”‚   â”œâ”€â”€ db/            # PostgreSQL models & session
â”‚   â”œâ”€â”€ vectorstore/   # Chroma interface
â”‚   â””â”€â”€ config/        # Settings & environment loading
â”‚
â”œâ”€â”€ worker/
â”‚   â””â”€â”€ worker.py      # Background ingestion worker
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ”Œ API Endpoints
Health
GET /health

Ingest Document
POST /ingest


Request

{
  "text": "Your document text here"
}


This triggers:

chunking

embedding generation

vector storage

metadata persistence

Processing is handled asynchronously by the worker.

Retrieve & Generate Answer
POST /retrieve


Request

{
  "query": "What is Retrieval-Augmented Generation?"
}


Response

Grounded answer generated from retrieved context

Retrieved context included for transparency

ğŸ—„ï¸ Persistence Model
Vector Store (Chroma)

Stores chunk embeddings

Used for semantic similarity search

Relational Database (PostgreSQL)

Stores ingestion jobs

Tracks processing status

Maintains structured system metadata

This dual-store approach avoids overloading the vector DB with system state.

âš™ï¸ Running Locally
Prerequisites

Docker

Docker Compose

Environment Variables

Create a .env file in the project root:

OPENAI_API_KEY=sk-or-xxxxxxxxxxxxxxxxxxxx


âš ï¸ Use an OpenRouter API key (sk-or-...), not a direct OpenAI key.

Start the system
docker compose up --build


Then open:

http://127.0.0.1:8000/docs

ğŸ§ª Verified Functionality

âœ… Document ingestion

âœ… Text chunking

âœ… Embeddings generation

âœ… Vector storage (Chroma)

âœ… Metadata layer (Postgres)

âœ… Semantic retrieval

âœ… Context-grounded generation

ğŸ§­ Roadmap Alignment

This project completes Step 4 â€“ Retrieval-Augmented Generation.

Step	Status
Document ingestion	âœ…
Text chunking	âœ…
Embeddings	âœ…
Vector store (Chroma)	âœ…
Metadata layer (Postgres)	âœ…
Retrieval	âœ…
Generation	âœ…
ğŸ”’ Freeze Notice

This repository is intentionally frozen at Step 4.

Future work such as:

evaluation

observability

tracing

guardrails

will be developed in a separate branch or repository.

ğŸ¯ Why This Project Exists

This project demonstrates the ability to:

Design AI systems end-to-end

Build production-style backends

Apply RAG correctly (not prompt stuffing)

Separate semantic memory from system state

Debug real LLM integration issues

It is intended as a learning milestone and portfolio project, not a full product.
